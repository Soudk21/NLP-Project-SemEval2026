{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 13559830,
          "sourceType": "datasetVersion",
          "datasetId": 8613082
        }
      ],
      "dockerImageVersionId": 31153,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "# Text Preprocessing & Baseline Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Transformer Model (PyTorch)\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import Trainer\n",
        "from transformers import TrainingArguments\n",
        "from datasets import load_dataset, Dataset as HfDataset\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "pjwXD2Gx0m0l"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in data\n",
        "file_path = \"/train_subtask1.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "df.info()\n",
        "print(df.head())"
      ],
      "metadata": {
        "trusted": true,
        "id": "3fH0U__60m0o"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# EDA - Check for Missing Values ---\n",
        "\n",
        "# Check for nulls, especially in the 'text' column\n",
        "print(f\"Null values in 'text' column: {df['text'].isnull().sum()}\")\n",
        "\n",
        "# For simplicity, we'll drop any rows where the text is missing, as we can't train on them.\n",
        "df.dropna(subset=['text'], inplace=True)\n",
        "print(f\"Shape after dropping null texts: {df.shape}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "0Z06h8xt0m0p"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# EDA - Analyze Target Variables (Valence & Arousal)\n",
        "\n",
        "# Set up the plotting area\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Plot 1: Valence Distribution\n",
        "sns.histplot(df['valence'], kde=True, ax=axes[0], bins=20)\n",
        "axes[0].set_title('Distribution of Valence Scores')\n",
        "\n",
        "# Plot 2: Arousal Distribution\n",
        "sns.histplot(df['arousal'], kde=True, ax=axes[1], bins=20)\n",
        "axes[1].set_title('Distribution of Arousal Scores')\n",
        "\n",
        "# Plot 3: Valence vs. Arousal\n",
        "# This helps us understand the relationship between our two target variables.\n",
        "sns.scatterplot(data=df, x='valence', y='arousal', ax=axes[2], alpha=0.3)\n",
        "axes[2].set_title('Valence vs. Arousal')\n",
        "plt.suptitle('EDA on Target Variables (Valence & Arousal)', fontsize=16, y=1.03)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "ejDdRMZn0m0p"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Train/Validation Split by User\n",
        "\n",
        "# Get a unique list of all user IDs\n",
        "all_user_ids = df['user_id'].unique()\n",
        "print(f\"Total unique users: {len(all_user_ids)}\")\n",
        "\n",
        "# Split the list of user IDs\n",
        "train_user_ids, val_user_ids = train_test_split(all_user_ids, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training users: {len(train_user_ids)}\")\n",
        "print(f\"Validation users: {len(val_user_ids)}\")\n",
        "\n",
        "# Create the final DataFrames\n",
        "train_df = df[df['user_id'].isin(train_user_ids)].copy()\n",
        "val_df = df[df['user_id'].isin(val_user_ids)].copy()\n",
        "\n",
        "print(f\"\\nTraining data shape: {train_df.shape}\")\n",
        "print(f\"Validation data shape: {val_df.shape}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "q5ZkllKi0m0r"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Model and Tokenizer\n",
        "# We'll use 'distilbert-base-uncased'. It's a smaller, faster version of BERT.\n",
        "MODEL_NAME = \"roberta-base\"\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "trusted": true,
        "id": "gkSjrpdP0m0s"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "token_lengths = [len(tokenizer(text)[\"input_ids\"]) for text in df[\"text\"]]\n",
        "max_token_length = max(token_lengths)\n",
        "print(max_token_length)"
      ],
      "metadata": {
        "id": "L9oJIJ1RT_FC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Data for Hugging Face `datasets`\n",
        "# The Trainer API works best with its own `Dataset` format.\n",
        "\n",
        "# We need to rename our label columns to 'labels' for the Trainer\n",
        "# And the model expects a *single* 'labels' tensor [valence, arousal]\n",
        "def format_data_for_hf(df):\n",
        "    df_hf = df[['text', 'valence', 'arousal']].copy()\n",
        "    # Create the 'labels' column as a list\n",
        "    df_hf['labels'] = df_hf.apply(lambda row: [row['valence'], row['arousal']], axis=1)\n",
        "    df_hf = df_hf.drop(columns=['valence', 'arousal'])\n",
        "    return df_hf\n",
        "\n",
        "train_hf_df = format_data_for_hf(train_df)\n",
        "val_hf_df = format_data_for_hf(val_df)\n",
        "\n",
        "# Convert pandas DataFrames to Hugging Face Dataset objects\n",
        "train_dataset = HfDataset.from_pandas(train_hf_df)\n",
        "val_dataset = HfDataset.from_pandas(val_hf_df)\n",
        "\n",
        "print(train_dataset)\n",
        "print(val_dataset[0])"
      ],
      "metadata": {
        "trusted": true,
        "id": "gr_X_x590m0s"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the Datasets ---\n",
        "# We create a function to tokenize the 'text' field\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    # max_length=128 is a good default for essays of this length.\n",
        "    # We truncate longer texts and pad shorter ones.\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=256)\n",
        "\n",
        "# Apply the tokenization to both datasets\n",
        "# batched=True makes this much faster\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Set the format to PyTorch tensors\n",
        "tokenized_train_dataset = tokenized_train_dataset.with_format(\"torch\")\n",
        "tokenized_val_dataset = tokenized_val_dataset.with_format(\"torch\")\n",
        "\n",
        "# Remove the original 'text' column as it's no longer needed\n",
        "tokenized_train_dataset = tokenized_train_dataset.remove_columns(['text'])\n",
        "tokenized_val_dataset = tokenized_val_dataset.remove_columns(['text'])\n",
        "\n",
        "print(\"\\n--- Tokenized Dataset Example ---\")\n",
        "print(tokenized_train_dataset[0])"
      ],
      "metadata": {
        "trusted": true,
        "id": "BdeSPB2y0m0t"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Model\n",
        "\n",
        "# We load 'AutoModelForSequenceClassification'\n",
        "# We tell it we have 2 labels (valence, arousal)\n",
        "# We MUST tell it this is a REGRESSION problem, not classification.\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=2,  # 2 outputs: valence, arousal\n",
        "    problem_type=\"regression\" # This is CRITICAL. It ensures we use MSELoss.\n",
        ")\n",
        "\n",
        "# If you have a GPU, move the model to the GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Model loaded on {device}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "9NZ6QU1Z0m0t"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Compute Metrics Function\n",
        "# The Trainer needs a function to calculate our Pearson Correlation metric\n",
        "# at the end of each epoch.\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    # eval_pred is a tuple of (logits, labels)\n",
        "    logits, labels = eval_pred\n",
        "\n",
        "    # Separate predictions and labels for valence and arousal\n",
        "    preds_v = logits[:, 0]\n",
        "    labels_v = labels[:, 0]\n",
        "\n",
        "    preds_a = logits[:, 1]\n",
        "    labels_a = labels[:, 1]\n",
        "\n",
        "    # Calculate Pearson Correlation for each\n",
        "    v_corr, _ = pearsonr(labels_v, preds_v)\n",
        "    a_corr, _ = pearsonr(labels_a, preds_a)\n",
        "\n",
        "    # Calculate the mean correlation (our main metric)\n",
        "    mean_corr = (v_corr + a_corr) / 2\n",
        "\n",
        "    # Also return MSE (what the model is optimizing)\n",
        "    mse = mean_squared_error(labels, logits)\n",
        "\n",
        "    return {\n",
        "        'pearson_r_valence': v_corr,\n",
        "        'pearson_r_arousal': a_corr,\n",
        "        'pearson_r_mean': mean_corr,\n",
        "        'mse': mse\n",
        "    }"
      ],
      "metadata": {
        "trusted": true,
        "id": "W06iTaY40m0t"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Training Arguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_roberta_stable\",\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=8,       # Keep this small to fit in memory\n",
        "    gradient_accumulation_steps=4,       # <--- INCREASED: Now effective batch size is 8 * 4 = 32!\n",
        "    per_device_eval_batch_size=32,\n",
        "    learning_rate=1e-5,                  # Keep this low\n",
        "    warmup_ratio=0.1,                    # Keep warmup\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"pearson_r_mean\",\n",
        "    greater_is_better=True,\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "MCAqM_Bb0m0t"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # The model to train\n",
        "    args=training_args,                  # Training arguments\n",
        "    train_dataset=tokenized_train_dataset, # Training dataset\n",
        "    eval_dataset=tokenized_val_dataset,    # Validation dataset\n",
        "    compute_metrics=compute_metrics      # The function to compute metrics\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "mK2QN07o0m0u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Model\n",
        "# This will take some time, especially if you are not on a GPU.\n",
        "print(\"Starting Transformer model training...\")\n",
        "trainer.train()\n",
        "print(\"Training complete.\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "cOjbh0Ee0m0u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Best Model\n",
        "print(\"Evaluating best model on validation set...\")\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(\"\\n--- Final Model Evaluation ---\")\n",
        "print(f\"Valence Pearson Correlation: {eval_results['eval_pearson_r_valence']:.4f}\")\n",
        "print(f\"Arousal Pearson Correlation: {eval_results['eval_pearson_r_arousal']:.4f}\")\n",
        "print(f\"**Mean Pearson Correlation:** {eval_results['eval_pearson_r_mean']:.4f}\")\n",
        "print(f\"Mean Squared Error (MSE): {eval_results['eval_mse']:.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "U1-lHFD70m0u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = val_df.copy()\n",
        "print(f\"Test data loaded. Shape: {test_df.shape}\")\n",
        "\n",
        "# Prepare the test data in the same way\n",
        "test_hf_df = test_df[['text_id', 'text']].copy()\n",
        "test_dataset_hf = HfDataset.from_pandas(test_hf_df)\n",
        "\n",
        "# Tokenize the test data\n",
        "tokenized_test_dataset = test_dataset_hf.map(tokenize_function, batched=True)\n",
        "tokenized_test_dataset = tokenized_test_dataset.with_format(\"torch\")\n",
        "tokenized_test_dataset = tokenized_test_dataset.remove_columns(['text'])\n",
        "\n",
        "print(tokenized_test_dataset)"
      ],
      "metadata": {
        "trusted": true,
        "id": "1fv5qGld0m0u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Predictions\n",
        "\n",
        "print(\"Generating predictions on test set...\")\n",
        "\n",
        "# The .predict() method returns an object with predictions, labels (if any), and metrics\n",
        "test_predictions = trainer.predict(tokenized_test_dataset)\n",
        "\n",
        "# The raw predictions (logits) are in the .predictions attribute\n",
        "pred_logits = test_predictions.predictions\n",
        "\n",
        "print(f\"Predictions shape: {pred_logits.shape}\") # Should be (num_test_samples, 2)"
      ],
      "metadata": {
        "trusted": true,
        "id": "83RFenVq0m0u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Format and Save Submission File\n",
        "\n",
        "# The required format is: text_id, pred_valence, pred_arousal\n",
        "\n",
        "# Extract predicted valence and arousal\n",
        "pred_valence = pred_logits[:, 0]\n",
        "pred_arousal = pred_logits[:, 1]\n",
        "\n",
        "# Create the submission DataFrame\n",
        "submission_df = pd.DataFrame({\n",
        "    'text_id': test_df['text_id'],\n",
        "    'pred_valence': pred_valence,\n",
        "    'pred_arousal': pred_arousal\n",
        "})\n",
        "\n",
        "# Define the submission file name\n",
        "submission_filename = \"pred_subtask1.csv\"\n",
        "\n",
        "# Save to CSV, index=False is required by the competition\n",
        "submission_df.to_csv(submission_filename, index=False)\n",
        "\n",
        "print(f\"\\nSuccessfully created submission file: {submission_filename}\")\n",
        "print(submission_df.head())"
      ],
      "metadata": {
        "trusted": true,
        "id": "MC9gYUaR0m0u"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}