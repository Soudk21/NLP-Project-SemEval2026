{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23c04a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Load and Clean Data ---\n",
      "Successfully loaded 'TRAIN_RELEASE_3SEP2025/train_subtask2a.csv'\n",
      "Original number of rows: 2764\n",
      "Rows with missing 'state_change_valence': 137\n",
      "Number of rows after removing missing answers: 2627\n",
      "\n",
      "--- First 5 rows of CLEANED data ---\n",
      "   user_id  text_id                                               text  \\\n",
      "0        1      200  I feel good .   I caught up on some sleep . Wo...   \n",
      "1        1      201  I’ve been feeling good for days and days . I r...   \n",
      "2        1      202  I’ve been feeling fine personally . I’ve been ...   \n",
      "3        1      203  I feel great . I’ve had a day off . I’m going ...   \n",
      "5        2       23       Productive , Tired , Active , Pleased , Busy   \n",
      "\n",
      "             timestamp  collection_phase  is_words  valence  arousal  \\\n",
      "0  2021-06-09 12:41:57                 1     False      2.0      1.0   \n",
      "1  2021-06-11 12:01:45                 1     False      2.0      1.0   \n",
      "2  2021-06-13 13:15:07                 1     False      0.0      1.0   \n",
      "3  2021-06-16 12:03:12                 1     False      2.0      1.0   \n",
      "5  2021-06-08 15:11:55                 1      True      0.0      2.0   \n",
      "\n",
      "   state_change_valence  state_change_arousal  \n",
      "0                   0.0                   0.0  \n",
      "1                  -2.0                   0.0  \n",
      "2                   2.0                   0.0  \n",
      "3                   0.0                   1.0  \n",
      "5                   0.0                   0.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np # pandas uses numpy, so it's good to import\n",
    "\n",
    "print(\"Load and Clean Data\")\n",
    "\n",
    "# Define the file path\n",
    "file_path = 'TRAIN_RELEASE_3SEP2025/train_subtask2a.csv'\n",
    "\n",
    "try:\n",
    "    # 1. Load the dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded '{file_path}'\")\n",
    "    print(f\"Original number of rows: {len(df)}\")\n",
    "\n",
    "    # 2. Check for missing \"answers\" (our target columns)\n",
    "    valence_nulls = df['state_change_valence'].isnull().sum()\n",
    "    print(f\"Rows with missing 'state_change_valence': {valence_nulls}\")\n",
    "    \n",
    "    # 3. Remove rows where *either* of the target columns is null\n",
    "    # We create a new, clean DataFrame\n",
    "    df_clean = df.dropna(subset=['state_change_valence', 'state_change_arousal'])\n",
    "\n",
    "    print(f\"Number of rows after removing missing answers: {len(df_clean)}\")\n",
    "\n",
    "    # Let's look at the first 5 rows of our new clean data\n",
    "    print(\"\\n--- First 5 rows of CLEANED data ---\")\n",
    "    print(df_clean.head())\n",
    "\n",
    "    # IMPORTANT: From now on, we will only use the 'df_clean' DataFrame.\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: The file '{file_path}' was not found.\")\n",
    "    print(\"Please make sure 'train_subtask2a.csv' is in the same folder as your Python script.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a6d634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Baby Step 2: Cleaning the Text ---\n",
      "Applying cleaning to all rows...\n",
      "--- 'text' vs 'text_clean' in our DataFrame ---\n",
      "                                                text  \\\n",
      "0  I feel good .   I caught up on some sleep . Wo...   \n",
      "1  I’ve been feeling good for days and days . I r...   \n",
      "2  I’ve been feeling fine personally . I’ve been ...   \n",
      "3  I feel great . I’ve had a day off . I’m going ...   \n",
      "5       Productive , Tired , Active , Pleased , Busy   \n",
      "\n",
      "                                          text_clean  \n",
      "0  i feel good i caught up on some sleep work wen...  \n",
      "1  ive been feeling good for days and days i real...  \n",
      "2  ive been feeling fine personally ive been tryi...  \n",
      "3  i feel great ive had a day off im going to go ...  \n",
      "5               productive tired active pleased busy  \n",
      "\n",
      "Step 2 Complete. We now have a 'text_clean' column.\n"
     ]
    }
   ],
   "source": [
    "import re # Import the regular expressions library\n",
    "import warnings\n",
    "\n",
    "# Suppress that annoying SettingWithCopyWarning\n",
    "warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "\n",
    "print(\"Cleaning the Text\")\n",
    "\n",
    "# This 'try' block will only run if 'df_clean' exists from Step 1\n",
    "try:\n",
    "    \n",
    "    # Define a function to clean our text\n",
    "    def clean_text(text):\n",
    "        # 1. Make the text lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # 2. Remove anything that is not a-z or a space\n",
    "        # [^a-z\\s] means \"match anything that is NOT (^) a-z or whitespace (\\s)\"\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "        \n",
    "        # 3. Remove extra spaces\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    # --- Now, apply this function to our entire 'text' column ---\n",
    "    # We use .loc to create the new column safely without a warning\n",
    "    print(\"Applying cleaning to all rows...\")\n",
    "    df_clean.loc[:, 'text_clean'] = df_clean['text'].apply(clean_text)\n",
    "    \n",
    "    # --- Let's look at the result ---\n",
    "    print(\"--- 'text' vs 'text_clean' in our DataFrame ---\")\n",
    "    print(df_clean[['text', 'text_clean']].head())\n",
    "    print(\"\\nStep 2 Complete. We now have a 'text_clean' column.\")\n",
    "\n",
    "\n",
    "except NameError:\n",
    "    print(\"\\nERROR: The 'df_clean' DataFrame was not found.\")\n",
    "    print(\"Please make sure you ran Baby Step 1 successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ba4941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Baby Step 3: Splitting into Train/Validation ---\n",
      "Total rows in our clean data:    2627\n",
      "Rows in our new 'Training' set:  2101\n",
      "Rows in our new 'Validation' set: 526\n",
      "\n",
      "--- Example of a 'Training' text (index 0) ---\n",
      "i am currently feeling tired i woke up very early for my shift at the hospital and did not have a good night s sleep i am looking forward to see my family tonight in order to celebrate my birthday i am happy they live nearby and will be able to visit\n",
      "\n",
      "--- Example of a 'Training' answer (index 0) ---\n",
      "state_change_valence    0.0\n",
      "state_change_arousal    0.0\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "Step 3 Complete. We now have X_train, y_train, X_val, y_val.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split # Import the splitter\n",
    "\n",
    "print(\"Splitting into Train/Validation\")\n",
    "\n",
    "# This 'try' block will only run if 'df_clean' exists\n",
    "try:\n",
    "    \n",
    "    # 1. Define our \"X\" (inputs) and \"y\" (answers)\n",
    "    \n",
    "    # X (input) is JUST the clean text\n",
    "    X = df_clean['text_clean']\n",
    "    \n",
    "    # y (answer) is our two 'state_change' columns\n",
    "    y = df_clean[['state_change_valence', 'state_change_arousal']]\n",
    "    \n",
    "    \n",
    "    # 2. Split the data!\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, \n",
    "        y, \n",
    "        test_size=0.2, \n",
    "        random_state=42 # for reproducible results\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # 3. --- CRITICAL FIX: Reset the Indexes ---\n",
    "    # This ensures our indexes go 0, 1, 2... which is\n",
    "    # required for the PyTorch Dataset in the next steps.\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    X_val = X_val.reset_index(drop=True)\n",
    "    y_val = y_val.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    # 4. Let's see the results\n",
    "    print(f\"Total rows in our clean data:    {len(df_clean)}\")\n",
    "    print(f\"Rows in our new 'Training' set:  {len(X_train)}\")\n",
    "    print(f\"Rows in our new 'Validation' set: {len(X_val)}\")\n",
    "    \n",
    "    print(\"\\n--- Example of a 'Training' text (index 0) ---\")\n",
    "    print(X_train.iloc[0])\n",
    "    \n",
    "    print(\"\\n--- Example of a 'Training' answer (index 0) ---\")\n",
    "    print(y_train.iloc[0])\n",
    "    \n",
    "    print(\"\\nStep 3 Complete. We now have X_train, y_train, X_val, y_val.\")\n",
    "\n",
    "except NameError:\n",
    "    print(\"\\nERROR: The 'df_clean' DataFrame was not found.\")\n",
    "    print(\"Please make sure you ran Baby Step 1 & 2 successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fffb728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Advanced Step 1: Load the Tokenizer ---\n",
      "Loading tokenizer for 'distilbert-base-uncased'...\n",
      "Tokenizer loaded successfully.\n",
      "\n",
      "--- Testing tokenizer on: 'i feel great today' ---\n",
      "Text: i feel great today\n",
      "Tokens: ['i', 'feel', 'great', 'today']\n",
      "Input IDs: [101, 1045, 2514, 2307, 2651, 102]\n",
      "\n",
      "Step 4 (Advanced 1) Complete. We now have our 'tokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "print(\"Load the Tokenizer\")\n",
    "\n",
    "# This is our \"Champion\" model\n",
    "# We define it here so all the next steps can use it\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "\n",
    "try:\n",
    "    # 1. Load the matching tokenizer\n",
    "    print(f\"Loading tokenizer for '{MODEL_NAME}'...\")\n",
    "    \n",
    "    # use_safetensors=True is good practice for security and speed\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_safetensors=True)\n",
    "    \n",
    "    print(\"Tokenizer loaded successfully.\")\n",
    "    \n",
    "    # 2. --- Let's test the tokenizer ---\n",
    "    test_text = \"i feel great today\" # A simple, clean example\n",
    "    \n",
    "    print(f\"\\n--- Testing tokenizer on: '{test_text}' ---\")\n",
    "    \n",
    "    # This shows what the tokenizer does:\n",
    "    # It chops the text into \"tokens\" (word-pieces)\n",
    "    # And converts them to \"input_ids\" (numbers)\n",
    "    tokenized_output = tokenizer(test_text)\n",
    "    \n",
    "    print(f\"Text: {test_text}\")\n",
    "    print(f\"Tokens: {tokenizer.tokenize(test_text)}\")\n",
    "    print(f\"Input IDs: {tokenized_output['input_ids']}\")\n",
    "    \n",
    "    print(\"\\nStep 4 (Advanced 1) Complete. We now have our 'tokenizer'.\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\nERROR: 'transformers' library not found.\")\n",
    "    print(\"Please make sure you have it installed: pip install transformers\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c75d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Advanced Step 2: Creating the *Simple* PyTorch Dataset ---\n",
      "Creating the training dataset...\n",
      "Creating the validation dataset...\n",
      "Datasets created successfully.\n",
      "\n",
      "--- Grabbing one item (index 0) from train_dataset ---\n",
      "Item's dictionary keys:\n",
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "Shape of 'input_ids': torch.Size([128])\n",
      "Shape of 'labels':    torch.Size([2])\n",
      "\n",
      "Step 5 (Advanced 2) Complete. We have 'train_dataset' and 'val_dataset'.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "print(\"Creating the *Simple* PyTorch Dataset\")\n",
    "\n",
    "# This 'try' block will only run if our previous steps exist\n",
    "try:\n",
    "    \n",
    "    # 1. DEFINE OUR SIMPLE DATASET CLASS\n",
    "    \n",
    "    class EmotionDataset(Dataset):\n",
    "        # This dataset only loads text and labels. Nice and simple.\n",
    "        def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "            self.texts = texts\n",
    "            self.labels = labels\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_len = max_len\n",
    "\n",
    "        # This tells PyTorch how many items are in the dataset\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "\n",
    "        # This tells PyTorch *how to get* a single item\n",
    "        def __getitem__(self, idx):\n",
    "            \n",
    "            # 1. Get the text at the index (e.g., index 0)\n",
    "            # We use .iloc[idx] because we reset our indexes in Step 3\n",
    "            text = self.texts.iloc[idx]\n",
    "            \n",
    "            # 2. Get the label row at the index (e.g., index 0)\n",
    "            # .values turns the DataFrame row into a numpy array [val, ars]\n",
    "            label = self.labels.iloc[idx].values \n",
    "            \n",
    "            # 3. Tokenize the text\n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=self.max_len,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # 4. Return our simple dictionary\n",
    "            return {\n",
    "                # .squeeze() removes extra dimensions (e.g., [1, 128] -> [128])\n",
    "                'input_ids': encoding['input_ids'].squeeze(),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "                'labels': torch.tensor(label, dtype=torch.float32)\n",
    "            }\n",
    "\n",
    "    \n",
    "    # 2. CREATE THE DATASET OBJECTS!\n",
    "    # We use the X_train, y_train, etc. from Step 3\n",
    "    \n",
    "    print(\"Creating the training dataset...\")\n",
    "    train_dataset = EmotionDataset(\n",
    "        texts=X_train,\n",
    "        labels=y_train,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    print(\"Creating the validation dataset...\")\n",
    "    val_dataset = EmotionDataset(\n",
    "        texts=X_val,\n",
    "        labels=y_val,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    print(\"Datasets created successfully.\")\n",
    "    \n",
    "    # 3. --- Let's test it! ---\n",
    "    print(\"\\n--- Grabbing one item (index 0) from train_dataset ---\")\n",
    "    item = train_dataset[0]\n",
    "    \n",
    "    print(\"Item's dictionary keys:\")\n",
    "    print(item.keys())\n",
    "    print(f\"\\nShape of 'input_ids': {item['input_ids'].shape}\")\n",
    "    print(f\"Shape of 'labels':    {item['labels'].shape}\")\n",
    "    \n",
    "    print(\"\\nStep 5 (Advanced 2) Complete. We have 'train_dataset' and 'val_dataset'.\")\n",
    "\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"\\nERROR: A variable was not found. Did you run all previous steps? {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad02406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Advanced Step 3: Creating the DataLoaders ---\n",
      "Creating train_loader with batch_size=16 and shuffling...\n",
      "Creating val_loader with batch_size=16...\n",
      "\n",
      "DataLoaders are created!\n",
      "\n",
      "--- Grabbing one 'batch' from the train_loader ---\n",
      "Shape of 'input_ids' batch: torch.Size([16, 128])\n",
      "Shape of 'attention_mask' batch: torch.Size([16, 128])\n",
      "Shape of 'labels' batch:    torch.Size([16, 2])\n",
      "\n",
      "This means the batch has 16 items (our batch size),\n",
      "and each item has 128 token IDs.\n",
      "\n",
      "Step 6 (Advanced 3) Complete. We have 'train_loader' and 'val_loader'.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(\"Creating the DataLoaders\")\n",
    "\n",
    "# This 'try' block will only run if our datasets exist\n",
    "try:\n",
    "    \n",
    "    # 1. DEFINE A BATCH SIZE\n",
    "    # This is a \"hyperparameter\" you can tune.\n",
    "    # 16 is a very common and safe batch size.\n",
    "    # It means \"feed the model 16 items at a time.\"\n",
    "    BATCH_SIZE = 16\n",
    "    \n",
    "    \n",
    "    # 2. CREATE THE TRAIN DATALOADER\n",
    "    print(f\"Creating train_loader with batch_size={BATCH_SIZE} and shuffling...\")\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True  # Shuffle the training data!\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # 3. CREATE THE VALIDATION DATALOADER\n",
    "    print(f\"Creating val_loader with batch_size={BATCH_SIZE}...\")\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False # No need to shuffle validation data\n",
    "    )\n",
    "    \n",
    "    \n",
    "    print(\"\\nDataLoaders are created!\")\n",
    "    \n",
    "    \n",
    "    # 4. --- Let's test it! ---\n",
    "    print(\"\\n--- Grabbing one 'batch' from the train_loader ---\")\n",
    "    \n",
    "    # 'iter' creates an iterator, 'next' grabs the first item\n",
    "    data_batch = next(iter(train_loader))\n",
    "    \n",
    "    # Let's check the shapes. Now they have a \"batch\" dimension!\n",
    "    print(f\"Shape of 'input_ids' batch: {data_batch['input_ids'].shape}\")\n",
    "    print(f\"Shape of 'attention_mask' batch: {data_batch['attention_mask'].shape}\")\n",
    "    print(f\"Shape of 'labels' batch:    {data_batch['labels'].shape}\")\n",
    "    \n",
    "    print(f\"\\nThis means the batch has {data_batch['input_ids'].shape[0]} items (our batch size),\")\n",
    "    print(f\"and each item has {data_batch['input_ids'].shape[1]} token IDs.\")\n",
    "    \n",
    "    print(\"\\nStep 6 (Advanced 3) Complete. We have 'train_loader' and 'val_loader'.\")\n",
    "    \n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"\\nERROR: A variable was not found. Did you run all previous steps? {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c10447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Advanced Step 4: Defining the *Simple* AI Model ---\n",
      "Using device: cuda\n",
      "Creating an instance of EmotionRegressorModel...\n",
      "\n",
      "--- MODEL CREATED SUCCESSFULLY! ---\n",
      "The 'regression_head' now expects 768 input features.\n",
      "\n",
      "Step 7 (Advanced 4) Complete. We have 'advanced_model' and 'device'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "\n",
    "print(\"Defining the *Simple* AI Model\")\n",
    "\n",
    "# This 'try' block will only run if our previous steps exist\n",
    "try:\n",
    "    \n",
    "    # 1. DEFINE OUR SIMPLE MODEL CLASS\n",
    "    # We inherit from torch.nn.Module, the base class for all PyTorch models\n",
    "    \n",
    "    class EmotionRegressorModel(nn.Module):\n",
    "        \n",
    "        # The 'init' runs once when we create the model\n",
    "        def __init__(self, model_name):\n",
    "            super(EmotionRegressorModel, self).__init__()\n",
    "            \n",
    "            # --- Part 1: The \"Body\" ---\n",
    "            # Load the pre-trained DistilBERT model\n",
    "            # This is the \"brain\" that understands language\n",
    "            self.bert_body = AutoModel.from_pretrained(model_name, use_safetensors=True)\n",
    "            \n",
    "            # --- Part 2: The \"Head\" ---\n",
    "            # We add our custom \"adapter\" on top of the body\n",
    "            # DistilBERT's output is 768 features (self.bert_body.config.hidden_size)\n",
    "            # We want to map this down to 2 outputs (valence and arousal)\n",
    "            self.regression_head = nn.Linear(\n",
    "                self.bert_body.config.hidden_size, # Input: 768\n",
    "                2                                  # Output: 2\n",
    "            )\n",
    "        \n",
    "        # The 'forward' method defines how data flows *through* the model\n",
    "        def forward(self, input_ids, attention_mask):\n",
    "            \n",
    "            # 1. Pass the data through the \"body\"\n",
    "            # We get back all the \"hidden states\" (the model's \"thoughts\")\n",
    "            outputs = self.bert_body(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            # 2. Get the *first* \"thought\"\n",
    "            # We use the output of the very first [CLS] token\n",
    "            # This 'last_hidden_state[:, 0]' is a standard way to get a\n",
    "            # single vector that represents the *entire* sentence.\n",
    "            cls_output = outputs.last_hidden_state[:, 0]\n",
    "            \n",
    "            # 3. Pass that one vector through our \"head\"\n",
    "            predictions = self.regression_head(cls_output)\n",
    "            \n",
    "            # 4. Return the final 2-number prediction\n",
    "            return predictions\n",
    "\n",
    "    \n",
    "    # 2. CHECK FOR A GPU (THIS IS IMPORTANT!)\n",
    "    # Training on a GPU is 100x faster than a CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    \n",
    "    # 3. CREATE AN INSTANCE OF OUR MODEL\n",
    "    print(\"Creating an instance of EmotionRegressorModel...\")\n",
    "    # This will load the pre-trained 'distilbert-base-uncased' weights\n",
    "    # (This may take a moment to download if it's your first time)\n",
    "    advanced_model = EmotionRegressorModel(model_name=MODEL_NAME)\n",
    "    \n",
    "    # 4. MOVE THE MODEL TO THE GPU (if we have one)\n",
    "    advanced_model.to(device)\n",
    "    \n",
    "    print(\"\\n--- MODEL CREATED SUCCESSFULLY! ---\")\n",
    "    print(f\"The 'regression_head' now expects {advanced_model.bert_body.config.hidden_size} input features.\")\n",
    "    \n",
    "    print(\"\\nStep 7 (Advanced 4) Complete. We have 'advanced_model' and 'device'.\")\n",
    "    \n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"\\nERROR: A variable was not found. Did you run all previous steps? {e}\")\n",
    "except ImportError:\n",
    "    print(\"\\nERROR: 'transformers' or 'torch' library not found.\")\n",
    "    print(\"Please make sure you have it installed: pip install transformers torch\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11953afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Advanced Step 5: Defining Loss and Optimizer ---\n",
      "Loss function (MSE) and Optimizer (AdamW) are created.\n",
      "We will train for 3 epochs.\n",
      "Our learning rate is 1e-05.\n",
      "\n",
      "Step 8 (Advanced 5) Complete. We are 100% ready to train.\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "import torch.nn as nn # We need this for the loss function\n",
    "\n",
    "print(\"Defining Loss and Optimizer\")\n",
    "\n",
    "# This 'try' block will only run if our model and device exist\n",
    "try:\n",
    "    \n",
    "    # -----------------------------------------------------------------\n",
    "    # --- TUNING KNOBS: This is where we \"fix parameters\"! ---\n",
    "    # -----------------------------------------------------------------\n",
    "    # We'll start with the settings that gave us our first good score\n",
    "    LEARNING_RATE = 1e-5  # (0.00001)\n",
    "    NUM_EPOCHS = 3        # We'll run 3 full passes to start\n",
    "    # -----------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    # 1. DEFINE THE \"MEASURING TAPE\" (LOSS FUNCTION)\n",
    "    loss_function = nn.MSELoss()\n",
    "    \n",
    "    \n",
    "    # 2. DEFINE THE \"WRENCH\" (OPTIMIZER)\n",
    "    # We tell AdamW *what* to fix (advanced_model.parameters())\n",
    "    # and *how much* to fix it (lr=LEARNING_RATE)\n",
    "    optimizer = AdamW(\n",
    "        advanced_model.parameters(),\n",
    "        lr=LEARNING_RATE\n",
    "    )\n",
    "    \n",
    "    print(\"Loss function (MSE) and Optimizer (AdamW) are created.\")\n",
    "    print(f\"We will train for {NUM_EPOCHS} epochs.\")\n",
    "    print(f\"Our learning rate is {LEARNING_RATE}.\")\n",
    "    \n",
    "    print(\"\\nStep 8 (Advanced 5) Complete. We are 100% ready to train.\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"\\nERROR: A variable was not found. Did you run all previous steps? {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e098f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Advanced Step 6: The Training & Evaluation Loop (FIXED) ---\n",
      "(This cell will *only* train the model. Saving is in the next cell.)\n",
      "\n",
      "--- Epoch 1 / 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Training Batches: 100%|██████████| 132/132 [00:06<00:00, 20.03it/s]\n",
      "  Validation Batches: 100%|██████████| 33/33 [00:00<00:00, 67.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Complete:\n",
      "  Average Training Loss: 1.4959\n",
      "  Average Validation Loss: 1.4978\n",
      "  (This is the new best validation loss so far!)\n",
      "\n",
      "--- Epoch 2 / 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Training Batches: 100%|██████████| 132/132 [00:06<00:00, 20.05it/s]\n",
      "  Validation Batches: 100%|██████████| 33/33 [00:00<00:00, 68.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Complete:\n",
      "  Average Training Loss: 1.3587\n",
      "  Average Validation Loss: 1.4366\n",
      "  (This is the new best validation loss so far!)\n",
      "\n",
      "--- Epoch 3 / 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Training Batches: 100%|██████████| 132/132 [00:06<00:00, 20.09it/s]\n",
      "  Validation Batches: 100%|██████████| 33/33 [00:00<00:00, 68.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Complete:\n",
      "  Average Training Loss: 1.2517\n",
      "  Average Validation Loss: 1.4444\n",
      "\n",
      "--- Training Complete! ---\n",
      "Calculating final scores using the BEST epoch's results...\n",
      "\n",
      "--- FINAL ADVANCED MODEL RESULTS ---\n",
      "  (Model: distilbert-base-uncased, LR: 1e-05, Epochs: 3)\n",
      "------------------------------------------\n",
      "--- Pearson's Correlation (r) ---\n",
      "  Valence: 0.3079\n",
      "  Arousal: 0.3223\n",
      "\n",
      "Step 9 (Advanced 6) Complete.\n",
      "The variable 'advanced_model' in memory is now fully trained.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm # For our nice progress bars!\n",
    "from scipy.stats import pearsonr # For our final score!\n",
    "\n",
    "print(\"The Training & Evaluation Loop\")\n",
    "\n",
    "# This 'try' block will only run if everything else exists\n",
    "try:\n",
    "    \n",
    "    # We'll use these to track our best score at the end\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch_preds = []\n",
    "    best_epoch_labels = []\n",
    "    \n",
    "    # 1. --- THE MAIN LOOP ---\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        \n",
    "        print(f\"\\n--- Epoch {epoch + 1} / {NUM_EPOCHS} ---\")\n",
    "        \n",
    "        # --- TRAINING PHASE ---\n",
    "        advanced_model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=\"  Training Batches\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = advanced_model(input_ids, attention_mask)\n",
    "            loss = loss_function(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        \n",
    "        # --- VALIDATION PHASE ---\n",
    "        advanced_model.eval()\n",
    "        total_val_loss = 0\n",
    "        current_preds = []\n",
    "        current_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"  Validation Batches\"):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                predictions = advanced_model(input_ids, attention_mask)\n",
    "                loss = loss_function(predictions, labels)\n",
    "                total_val_loss += loss.item()\n",
    "                \n",
    "                current_preds.append(predictions.cpu().numpy())\n",
    "                current_labels.append(labels.cpu().numpy())\n",
    "                \n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1} Complete:\")\n",
    "        print(f\"  Average Training Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # We just check the loss, we don't save the model here\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            print(\"  (This is the new best validation loss so far!)\")\n",
    "            # We'll save the *predictions* from this best epoch\n",
    "            best_epoch_preds = current_preds\n",
    "            best_epoch_labels = current_labels\n",
    "            \n",
    "    # 2. --- FINAL EVALUATION (AFTER ALL EPOCHS) ---\n",
    "    print(\"\\n--- Training Complete! ---\")\n",
    "    print(\"Calculating final scores using the BEST epoch's results...\")\n",
    "    \n",
    "    # 'vstack' stacks all our saved batches vertically into one big array\n",
    "    final_preds = np.vstack(best_epoch_preds)\n",
    "    final_labels = np.vstack(best_epoch_labels)\n",
    "    \n",
    "    real_valence = final_labels[:, 0]\n",
    "    real_arousal = final_labels[:, 1]\n",
    "    pred_valence = final_preds[:, 0]\n",
    "    pred_arousal = final_preds[:, 1]\n",
    "    \n",
    "    # 3. CALCULATE PEARSON'S R!\n",
    "    r_valence = pearsonr(real_valence, pred_valence)[0]\n",
    "    r_arousal = pearsonr(real_arousal, pred_arousal)[0]\n",
    "    \n",
    "    print(\"\\n--- FINAL ADVANCED MODEL RESULTS ---\")\n",
    "    print(f\"  (Model: {MODEL_NAME}, LR: {LEARNING_RATE}, Epochs: {NUM_EPOCHS})\")\n",
    "    print(\"------------------------------------------\")\n",
    "    print(\"--- Pearson's Correlation (r) ---\")\n",
    "    print(f\"  Valence: {r_valence:.4f}\")\n",
    "    print(f\"  Arousal: {r_arousal:.4f}\")\n",
    "    \n",
    "    print(\"The variable 'advanced_model' in memory is now fully trained.\")\n",
    "\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"\\nERROR: A variable was not found. Did you run all previous steps? {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
